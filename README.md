![image](https://github.com/user-attachments/assets/2b05c7dd-e4fd-4fae-85e1-654512b9e9e5)![image](https://github.com/user-attachments/assets/2b05c7dd-e4fd-4fae-85e1-654512b9e9e5)
An intelligent document Q&A chatbot built with **FastAPI**, **React**, **LangChain**, **HuggingFace Embeddings**, and **Groq's LLMs**.
This app allows you to upload a PDF and ask questions, with answers derived strictly from the document using **Retrieval-Augmented Generation (RAG)**.

![Document ChatBot Screenshot](https://user-images.githubusercontent.com/placeholder/document-chatbot.png)

## ğŸ”§ Features

- **ğŸ“¤ PDF Upload**: Upload PDF documents for analysis
- **ğŸ“ Text Extraction**: Extract text using PyMuPDF with OCR fallback (pytesseract)
- **ğŸ” Vector Search**: Store chunked documents as embeddings using HuggingFace + ChromaDB
- **ğŸ’¬ Natural Language Q&A**: Ask questions in plain English
- **ğŸ¤– Groq-Powered Responses**: Get accurate answers using **Groq's LLaMA 3-8B** model
- **ğŸ”„ Fallback Mechanism**: Intelligent fallback when vector search yields no results
- **ğŸ’¾ Persistent Storage**: ChromaDB for local vector storage

---

## ğŸš€ Tech Stack

### Frontend
- **React** + **Vite** for fast development
- **Axios** for API communication
- **Modern UI** with responsive design

### Backend
- **FastAPI** for high-performance API
- **LangChain** for RAG pipeline
- **Groq API** for LLM inference

### AI/ML Components
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`
- **Vector Database**: ChromaDB (persistent local storage)
- **LLM**: `llama3-8b-8192` via Groq
- **OCR Fallback**: pdf2image + pytesseract

---

## ğŸ“ Project Structure

```
document-chatbot/
â”‚
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ main.py                # Main FastAPI application
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ uploaded_files/        # PDF storage directory
â”‚   â””â”€â”€ db/                    # ChromaDB persistence
â”‚
â”œâ”€â”€ frontend/                   # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx           # Main React component
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â””â”€â”€ styles/           # CSS styles
â”‚   â”œâ”€â”€ public/               # Static assets
â”‚   â”œâ”€â”€ package.json          # Node.js dependencies
â”‚   â””â”€â”€ vite.config.js        # Vite configuration
â”‚
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ .gitignore                 # Git ignore rules
```

---

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- **Python 3.8+**
- **Node.js 16+**
- **Groq API Key** ([Get it here](https://console.groq.com/))

### 1ï¸âƒ£ Backend Setup (FastAPI)

```bash
# Navigate to backend directory
cd backend/

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt

# Alternative: Install dependencies manually
pip install fastapi uvicorn langchain langchain-groq langchain-community langchain-huggingface langchain-chroma pytesseract pdf2image pymupdf chromadb sentence-transformers
```

### 2ï¸âƒ£ System Dependencies

```bash
# Install Tesseract OCR
# On macOS:
brew install tesseract

# On Ubuntu/Debian:
sudo apt-get install tesseract-ocr

# On Windows:
# Download from: https://github.com/UB-Mannheim/tesseract/wiki

# Install Poppler (required by pdf2image)
# On macOS:
brew install poppler

# On Ubuntu/Debian:
sudo apt-get install poppler-utils

# On Windows:
# Download from: https://blog.alivate.com.au/poppler-windows/
```

### 3ï¸âƒ£ Environment Variables

Create a `.env` file in the backend directory:

```bash
# Backend/.env
GROQ_API_KEY=your_groq_api_key_here
```

Or set environment variable directly:

```bash
export GROQ_API_KEY="your_groq_api_key_here"
```

### 4ï¸âƒ£ Frontend Setup (React)

```bash
# Navigate to frontend directory
cd frontend/

# Install Node.js dependencies
npm install

# Alternative: Using yarn
yarn install
```

---

## ğŸš€ Running the Application

### Start Backend Server

```bash
cd backend/
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Backend will be available at: `http://localhost:8000`

### Start Frontend Development Server

```bash
cd frontend/
npm run dev
```

Frontend will be available at: `http://localhost:5173`

---

## ğŸ“– Usage

1. **Upload PDF**: Click the upload button and select a PDF document
2. **Wait for Processing**: The app will extract text and create embeddings
3. **Ask Questions**: Type your question in the chat interface
4. **Get Answers**: Receive contextual answers based on your document

### Example Questions

- "What is the main topic of this document?"
- "Can you summarize the key points?"
- "What does the document say about [specific topic]?"

---

## ğŸ”§ Configuration

### Backend Configuration

Edit `backend/main.py` to customize:

- **Chunk size**: Modify text splitting parameters
- **Embedding model**: Change HuggingFace model
- **LLM model**: Switch Groq model variant
- **Vector search**: Adjust similarity search parameters

### Frontend Configuration

Edit `frontend/src/App.jsx` to customize:

- **UI components**: Modify chat interface
- **API endpoints**: Update backend URLs
- **Styling**: Customize appearance

---

## ğŸ§ª API Endpoints

### Backend API

- `POST /upload/`: Upload PDF document
- `POST /ask/`: Ask question about uploaded document
- `GET /health/`: Health check endpoint

### Example API Usage

```bash
# Upload PDF
curl -X POST "http://localhost:8000/upload/" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@document.pdf"

# Ask question
curl -X POST "http://localhost:8000/ask/" \
     -H "Content-Type: application/json" \
     -d '{"question": "What is this document about?"}'
```

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---


## ğŸ‘¥ Authors

- **Your Name** - *Initial work* - https://github.com/suvhamdas12

---

## ğŸ™ Acknowledgments

- **Groq** for providing fast LLM inference
- **LangChain** for RAG framework
- **ChromaDB** for vector storage
- **HuggingFace** for embeddings
- **FastAPI** for the backend framework
- **React** for the frontend framework


**â­ Star this repository if you found it helpful!**
